{
  "name": "AI Advanced Techniques VIP1",
  "description": "Fine-tune, align, and integrate multimodal AI with precision and efficiency.",
  "entries": [
    {
      "id": "01",
      "title": "Fine-Tuning for Task Adaptation",
      "keywords_en": ["fine-tuning", "task", "pretrained", "weights"],
      "keywords_vi": ["tinh chỉnh", "tác vụ", "tiền huấn luyện", "trọng số"],
      "reply_en": "Update weights on new data to adapt BERT. Use small learning rates. Validate to avoid overfitting. Start with a tiny dataset.",
      "reply_vi": "Cập nhật trọng số trên dữ liệu mới để thích nghi BERT. Dùng tốc độ học nhỏ. Xác thực để tránh overfitting. Bắt đầu với tập dữ liệu nhỏ.",
      "dare": "Fine-tune one model parameter now.",
      "audio": "01_fine_tuning.mp3",
      "duration": "00:52"
    },
    {
      "id": "02",
      "title": "Low-Rank Adaptation (LoRA)",
      "keywords_en": ["LoRA", "low-rank", "efficient", "parameters"],
      "keywords_vi": ["LoRA", "hạng thấp", "tối ưu", "tham số"],
      "reply_en": "Add low-rank matrices to update models fast. Choose rank wisely. Apply LoRA to a language model for a custom task.",
      "reply_vi": "Thêm ma trận hạng thấp để cập nhật mô hình nhanh. Chọn hạng hợp lý. Áp dụng LoRA cho mô hình ngôn ngữ theo tác vụ riêng.",
      "dare": "Reduce one model’s size by 90%.",
      "audio": "02_lora_adaptation.mp3",
      "duration": "00:48"
    },
    {
      "id": "03",
      "title": "Reinforcement Learning with Feedback (RLHF)",
      "keywords_en": ["RLHF", "alignment", "reward", "human feedback"],
      "keywords_vi": ["RLHF", "căn chỉnh", "phần thưởng", "phản hồi"],
      "reply_en": "Train a reward model from human preferences. Use it to align AI. Test on a chatbot. Quality feedback is key.",
      "reply_vi": "Huấn luyện mô hình phần thưởng từ phản hồi người dùng. Dùng để căn chỉnh AI. Thử trên chatbot. Phản hồi chất lượng là chìa khóa.",
      "dare": "Rate one AI response: good or bad.",
      "audio": "03_rlhf.mp3",
      "duration": "00:50"
    },
    {
      "id": "04",
      "title": "Direct Preference Optimization (DPO)",
      "keywords_en": ["DPO", "preference", "pairwise", "alignment"],
      "keywords_vi": ["DPO", "sở thích", "cặp đôi", "căn chỉnh"],
      "reply_en": "Align AI using pairwise preferences. No reward model needed. Handle noisy data carefully. Try on a small preference set.",
      "reply_vi": "Căn chỉnh AI bằng cặp sở thích. Không cần mô hình phần thưởng. Xử lý dữ liệu nhiễu cẩn thận. Thử trên tập sở thích nhỏ.",
      "dare": "Pick: A or B — which is better?",
      "audio": "04_dpo.mp3",
      "duration": "00:47"
    },
    {
      "id": "05",
      "title": "Multimodal Data Integration",
      "keywords_en": ["multimodal", "text", "image", "audio", "fusion"],
      "keywords_vi": [
        "đa phương thức",
        "văn bản",
        "hình ảnh",
        "âm thanh",
        "hòa trộn"
      ],
      "reply_en": "Fuse text, image, and audio in one model. Bridge modality gaps. Try a simple video captioning task.",
      "reply_vi": "Kết hợp văn bản, hình ảnh, âm thanh trong một mô hình. Vượt qua khoảng cách phương thức. Thử tạo phụ đề video đơn giản.",
      "dare": "Describe a photo in one sentence.",
      "audio": "05_multimodal_ai.mp3",
      "duration": "00:49"
    },
    {
      "id": "06",
      "title": "Graph-Augmented Retrieval (Graph RAG)",
      "keywords_en": [
        "graph RAG",
        "knowledge graph",
        "retrieval",
        "structured"
      ],
      "keywords_vi": ["graph RAG", "đồ thị tri thức", "truy xuất", "cấu trúc"],
      "reply_en": "Use knowledge graphs to boost retrieval. Build a small graph. Outperforms standard RAG. Needs clean data.",
      "reply_vi": "Dùng đồ thị tri thức để tăng cường truy xuất. Xây dựng đồ thị nhỏ. Vượt trội RAG tiêu chuẩn. Cần dữ liệu sạch.",
      "dare": "Link two facts with a relationship.",
      "audio": "06_graph_rag.mp3",
      "duration": "00:51"
    }
  ]
}
